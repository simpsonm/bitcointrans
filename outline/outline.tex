\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{bm, bbm}
%\usepackage{mathtools}

\author{Matt Simpson}
\title{Outline of Ideas for BTC Transaction Frequency}
\begin{document}
\maketitle

\abstract{As Bitcoin's popularity rises, so does the number of transactions per block. Existing Bitcoin rules limit the number of transactions per block to 1 megabytesâ€™ worth. Recent average transaction rates have been about 40 percent of that limit. But because Bitcoin blocks are discovered according to a stochastic Poisson process, some blocks may reach the limit even if the average is well below the limit. This paper models the percentage of blocks that reach the 1MB limit under various average transaction rates. In our most basic model, we derive a formula for that percentage as a function of constant transaction rates. We also estimate how that percentage changes when transaction rates exhibit minute-to-minute variability along specified, empirically defensible distributions.}



\section{Motivation}
As Bitcoin becomes more popular, the transaction rate increases. Since existing Bitcoin rules limit the amount of transactions per block to one MB at some point the blocks will begin filling up. Bitcoin blocks are by design distributed according a Poisson process with a rate of discovery of one block every 10 minutes. Let $\tau$ index time in minutes, and $N(\tau)$ denote the number of blocks discovered by time $\tau$. Then for $\delta > 0$, the number of blocks discovered between time $\tau$ and time $\tau + \delta$ is Poisson distributed, that is $N(\tau + \delta) - N(\tau) \sim Poi(\delta/10)$. Let $k=1,2,\dots$ denote each of the blocks, in order. Then the amount of time between blocks is exponentially distributed. Let $\tau_k$ denote the the time when block $k$ arrives. Then $\delta_k \equiv \tau_{k} - \tau_{k-1} \stackrel{iid}{\sim} Exp(1/10)$ for $k=1,2,\dots,K$, where if $x\sim Exp(\lambda)$ it has the density $p(x) = \lambda e^{-x\lambda}$ for $x>0$ and $\lambda>0$ with mean $1/\lambda$. The longer $\delta_k$ the more time for transactions to add up before being added to the block chain as part of block $k$.

There is always some chance that a given block will fill up since arrival times are stochastic. Specifically, for the $k$'th block this probability is
\[
P\left(x_{\tau_k} - x_{\tau_{k-1}} > 1\right).
\]
This probability depends crucially on the process that $x_\tau$ follows. If we assume that $x_{\tau}$ is determinstic with a constant transaction rate of $\gamma$ MB/min, then 
\[
x_{\tau_k} - x_{\tau_{k-1}} = \gamma\delta_k \stackrel{ind}{\sim} Exp(1/\gamma 10)
\]
and the probability of a full block is available in closed form as
\[
P\left(x_{\tau_k} - x_{\tau_{k-1}} > 1\right) = \int_1^\infty 1/10\gamma e^{-1/10\gamma x}dx = e^{-1/10\gamma}.
\]

Clearly a constant transaction rate does not capture Bitcoin's behavior since it has been growing in popularity. With a non-constant and potentially stochastic transaction rate, the probability of a block filling up is no longer necessarily available in closed form. Using a Monte Carlo simulation of $N$ draws of the block sizes from the process $x_{\tau}$, we can approximate the probability of block $k$ filling up with
\[
\frac{1}{N}\sum_{i=1}^N\mathbbm{1}\left\{x^{(i)}_{\tau_{k}^{(i)}} - x^{(i)}_{\tau_{k-1}^{(i)}} > 1\right\}
\]
where $i$ indexes the Monte Carlo simulations of each of the random quantities inside the indicator function. If we let $\gamma$ denote the mean transaction rate, which is the same as the actual transaction rate in the constant, deterministic case, we can alter $\gamma$ to see how the probability of a full block changes under a variety of regimes.

A key question is whether the assumption of a constant transaction rate is approximately correct on a short enough time-scale. In order to test this assumption we fit a gamma distribution to the block sizes of various pieces of the block chain. We use the shape-rate parameterization of the gamma distribution so that if $x\sim G(\alpha,\beta)$, it has the density
\[
p(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-x\beta}
\]
for $x>0$ with mean $\alpha/\beta$ and variance $\alpha/\beta^2$. Since the $G(1,\beta)$ and $Exp(\beta)$ distributions are identical, if we find that $\alpha\neq 1$ then transaction rates cannot be constant. We focus on the most recent 5000 blocks in our database ending with block 303151 --- we are in the process of downloading the entire chain.  For priors we will assume $\log \alpha \sim N(0, 10)$ and $\beta \sim G(1, 1)$. This prior on $\alpha$ implies that the prior median is $\alpha=1$ so that the null hypothesis is favored relatively strongly by the prior. We fit the model to all 5000 blocks, the first 1000 blocks, the second 1000 blocks, etc., and the first 500 blocks, the second 500 blocks, etc. The results are in Table \ref{tab:gammafit}. 

% latex table generated in R 3.2.2 by xtable 1.7-4 package
% Mon Oct 12 10:44:18 2015
\begin{table}[ht]
\centering
\begin{tabular}{lrrr}
  \hline
 & Mean & 2.5\% & 97.5\% \\ 
  \hline
       $n=5000$ & 1.34 & 1.29 & 1.39 \\ 
$n=1000$, $g=1$ & 1.26 & 1.17 & 1.37 \\ 
$n=1000$, $g=2$ & 1.36 & 1.26 & 1.47 \\ 
$n=1000$, $g=3$ & 1.37 & 1.26 & 1.47 \\ 
$n=1000$, $g=4$ & 1.27 & 1.17 & 1.37 \\ 
$n=1000$, $g=5$ & 1.41 & 1.30 & 1.52 \\ 
 $n=500$, $g=1$ & 1.29 & 1.15 & 1.45 \\ 
 $n=500$, $g=2$ & 1.23 & 1.10 & 1.37 \\ 
 $n=500$, $g=3$ & 1.46 & 1.31 & 1.63 \\ 
 $n=500$, $g=4$ & 1.28 & 1.14 & 1.42 \\ 
 $n=500$, $g=5$ & 1.55 & 1.38 & 1.74 \\ 
 $n=500$, $g=6$ & 1.21 & 1.08 & 1.34 \\ 
 $n=500$, $g=7$ & 1.24 & 1.11 & 1.38 \\ 
 $n=500$, $g=8$ & 1.29 & 1.15 & 1.44 \\ 
 $n=500$, $g=9$ & 1.36 & 1.21 & 1.52 \\ 
 $n=500$, $g=10$ & 1.46 & 1.30 & 1.63 \\ 
 \hline
\end{tabular}
\caption{Posterior mean and 95\% credible intervals for $\alpha$ from fitting a gamma distribution to block sizes of the last 5000 blocks in our database. For sample sizes less than 5000, blocks were broken into groups of equal size and the model was fit separately to each group. Sample size is indicated by $n$, group id by $g$. Each model was fit in RStan\protect\footnotemark using 4 chains, each with 4000 iterations, 2000 of which used for warmup/burn in.}
\label{tab:gammafit}
\end{table}

For\footnotetext{Cite RStan} the full 5000 blocks, the posterior mean of $\alpha$ is 1.34 and the lower bound of the 95\% credible interval is 1.29, so we have strong evidence that $\alpha$ is nonzero and that a constant transaction rate is not appropriate. This is not surprising since Bitcoin usage has gone up over time, but on a short enough time scale usage may be approximately constant. A block is around 10 minutes on average, so 5000 blocks takes a little over a month on average. It is possible that on a short enough time-scale, constant transaction rates are more appropriate. So we fit the gamma distribution to 1000 blocks at a time --- representing about a week --- by breaking the original 5000 blocks into five groups. For each of these groups the 95\% credible interval does not contain one --- the lowest lower bound is 1.17. Similarly for groups of 500 blocks, each representing about 3.5 days, the lowest lower bound is about 1.08. So we can safely conclude that the transaction rate is not constant. A likely reason for this is seasonality in transaction rates --- Bitcoin trades are probably more likely to happen during certain times of day than others, based on when relevant markets are open and when the bulk of Bitcoin users are awake. The {\it mean} transaction rate averaging over e.g. seasonality is probably approximately constant for time-scales this small, however. In any case, transaction rates are nonconstant even on short time-scales, which motivates modeling them as such.

\section{The Model}

The first part of the model is determined for us since we know $\delta_k\stackrel{iid}{\sim} Exp(1/10)$ by the design of Bitcoin, but determining the how how the amount of transaction data is distributed requires more work. Crucial constraints on the models we can fit come from the data obtained from the block chain. Since Bitcoin is a distributed network without a home node, there is no official timestamp for when each block arrives on the chain. Instead, the block miner's local time is recorded. Two constraints on the reported time that the block was mined are built into the Bitcoin protocol. First, any block submitted to the chain with a local time less than the median of local times of the last 11 blocks is rejected. Second, any block with a local time greater than network adjusted time plus two hours is rejected, where network adjusted time is defined as the local time plus the median difference between local time and the time at all nodes connected to the local node with a maximum offset of 70 minutes. Because of the nature of the blocks' timestamps, occasionally a block has a timestamp which is earlier than one or more previous blocks in the chain. Essentially, we have measurement error on the measurement times, and the lower and upper bounds for valid timestamps define lower and upper bounds for the measurement error distribution. Unfortunately network adjusted time is not recorded in the blockchain, so the upper bound is not observed, but the lower bound is at our disposal.

The second major constraint that the block chain imposes on us is that transaction data is only measured once a block arrives. So at random measurement times which we only observe with error, we observe the amount of transaction data since the previous measurement time --- though we observe this without error. These two constraints motivate a class of models for modeling Bitcoin transaction data. Once again, let $\tau_k$ denote the actual time of the arrival of the $k$'th block, $\delta_k$ the amount of time between block $k$ and block $k-1$ so that $\tau_{k}=\tau_{k-1} + \delta_k$, and now let $t_k$ denote the observed arrival time in the blockchain. Further, let $x_\tau$ denote the amount data in the entire chain at time $\tau$ so that $x_{\tau_{k}} - x_{\tau_{k-1}}$ denotes the amount of data in block $k$. Now suppose the amount of transaction data in MBs is distributed according to some nondecreasing, infinitely divisible\footnote{Should we define?} stochastic process. Most of the interesting modeling choices come from modeling this stochastic process. Conventional Brownian motion is inappropriate since the increments should be nonnegative while geometric Brownian motion is a poor choice since it cannot start from zero. Instead, we consider the classes of non-negative infinite activity\footnote{Should we define?} L\'{e}vy processes, often called subordinators, with closed form solutions for $x_{\tau}$.\footnote{Cite the Levy process chapter here}. The stochastic process $\{x_{\tau}:\tau>0\}$ is a L\'{e}vy process if it is continous time and has independent and stationary increments with $x_0=0$. Brownian motion is the most common example. A nondecreasing example is the Gamma process where $x_{\tau + \delta} - x_{\tau} \sim G(\alpha \delta, \beta)$ for any time $\tau$ and positive increment $\delta$. 

While all L\'{e}vy processes are infinitely divisible, the gamma process is particularly convenient since the distribtion of $x_{\tau+\delta} - x_\tau$ and $x_\tau$ are both known in closed form as a consequence of the fact that if $x_1\sim G(\alpha_1,\beta)$ independent of $x_2\sim G(\alpha_2,\beta)$, then $x_1+x_2 \sim G(\alpha_1 + \alpha_2, \beta)$. Often L\'{e}vy processes are defined by considering an infinitely divisible distribution for $x_1$, but the distribution of $x_{\tau}$ for any $\tau>0$ need not be from the same class as $x_1$. The inverse Gaussian process is another L\'{e}vy process which has a closed form solution for the distribution of $x_\tau$. The inverse Gaussian density is
\[
p(x) = \left[\frac{\lambda}{2\pi x^3}\right]^{1/2}\exp\left[\frac{-\lambda(x - \mu)^2}{2\mu^2x}\right]
\]
for $x>0$ with mean parameter $\mu>0$, shape parameter $\lambda>0$, and variance $\mu^3/\lambda$. Then increments of the inverse Gaussian process are inverse Gaussian distributed, viz. $x_{\tau + \delta} - x_\tau \sim IG(\mu\delta,\lambda\delta^2)$.

It is attractive that we can write down the distribution of $x_{\tau + \delta} - x_\tau$ in closed form since the observation times are observed with error. This simplifies Markov chain Monte Carlo methods that may be used to fit the model by allowing us to exactly simulate $x_{\tau_{k-1} + \delta_k}$ without resorting to approximation methods. The parameters of the Gamma or inverse Gaussian distributions can also be further modeled as stochastic processes in order to capture e.g. seasonal variation or stochastic volatility, though most reasonable models on these parameters will not allow for closed form distributions of the parameter increments. We will use the gamma process since it is widely available in statistical packages, unlike the main alternative, the inverse Gaussian.

On the data level we have measurement error with a lower bound, motivating a class of models of the form 
\[
t_{k+1} |t_{1:k}, \tau_{1:K} \stackrel{ind}{\sim} p(t|\tau_{k+1}, \phi)\mathbbm{1}\left\{t > m_{k+1})\right\}
\]
where $p(t|\tau,\phi)$ is a class of probability densities on $t>0$ with centrality parameter $\tau$ and dispersion parameter $\phi$, and $m_k = \mathrm{median}(t_{k - 0:10})$. For example $\tau$ could be the mean or the mode and $\phi$ the variance or precision. This interpretation of these parameter applies for the {\it untruncated} distribution $p(t|\tau,\phi)$, but not necessarily for the condtional dsitribution of $t_k$. A natural choice here is another gamma distribution, since the untruncated distribution still must be non-negative. However the gamma causes numerical problems at times since it forces the latent measurement times, $\tau_k$ to enter the gamma function. A similar model that avoids numerical problems is the lognormal distribution, which we will use.

Combining these pieces we obtain the following model. For block $k=1,2,\dots,K$
\begin{align*}
t_k|t_{1:(k-1)}, \tau_{1:K} &\sim LN(\log(\tau_k) - \sigma^2/2, \sigma)\mathbbm{1}(t_k > m_k)\\
x_{\tau_{k-1} + \delta_k} - x_{\tau_{k-1}}|\tau_{1:K} & \stackrel{ind}{\sim} G(\beta\gamma\delta_k,\beta)\\
\delta_{k} = \tau_{k} - \tau_{k-1} &\stackrel{iid}{\sim} Exp(1/10)
\end{align*}
where $x\sim LN(\mu,\sigma)$ means $\log(x) \sim N(\mu,\sigma)$. To complete the model we assume $\sigma$, $\beta$, and $\gamma$ are independent in the prior with $\sigma \sim Cauchy(0, 2.5)$, $\beta \sim G(1, 1)$, and $\gamma \sim LN(0, 10)$. Table \ref{tab:modelfit} contains the results of fitting the model.

% latex table generated in R 3.2.2 by xtable 1.7-4 package
% Mon Oct 12 22:27:11 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrrrr}
  \hline
 & Mean & SD & SE(Mean) & 2.5\% & 25\% & 50\% & 75\% & 97.5\% \\ 
  \hline
$\gamma$ & 0.0240 & 0.0004 & 0.0000 & 0.0232 & 0.0237 & 0.0240 & 0.0243 & 0.0248 \\ 
  $\beta$ & 15.0951 & 0.9228 & 0.0103 & 13.3725 & 14.4461 & 15.0760 & 15.6962 & 16.9591 \\ 
  $\sigma$ & 0.0024 & 0.0001 & 0.0000 & 0.0023 & 0.0023 & 0.0024 & 0.0024 & 0.0025 \\ 
   \hline
\end{tabular}
 \caption{Results of fitting the measurement error model with stochastic measurement times. We use RStan to fit the model, obtaining four chains each of 4000 iterations, 2000 of which used for warmup/burn in.\protect\footnotemark (Note: outdated numbers, updated ones coming) }
 \label{tab:modelfit}
\end{table}

%% \subsection{Stochastic transaction rates --- seasonality and autocorrelation}
%% It is unlikely that the transaction rate is constant in time. The transaction rate is clearly growing as Bitcoin becomes more popular, and seasonal variation is expected throughout the data. Further, autocorrelation in transaction rates is likely since periods of high activity will sometimes be caused by, e.g., new information about the long term prospects of Bitcoin.

%% Let $\gamma_\tau$ denote the transaction rate at time $\tau$. We model the evolution of this rate as a variant of the exponential Ornsteinâ€“Uhlenbeck (OU) process. Specifically let $\varepsilon_t = \log(\gamma_t) - \mu_t$ , and let $\mu_t$ define a deterministic function of time containing seasonal and trend components. Then $\varepsilon_{\tau}$ is an OU process:
%% \begin{align*}
%% d\varepsilon_{\tau} = -\phi \varepsilon_{\tau}d\tau + \sigma_{\varepsilon} dB_{\tau}
%% \end{align*}
%% where $B_\tau$ is standard Brownian motion. It\^{o}'s lemma gives the stochastic differential equation for $\gamma_{\tau} = e^{\varepsilon_{\tau} + \mu_{\tau}}$:
%% \begin{align*}
%% d\gamma_\tau = -\phi(\log\gamma_\tau - \mu_\tau)\gamma_\tau + \sigma_{\varepsilon}^2/2\gamma_\tau +d\mu_\tau \gamma_\tau + \sigma_{\varepsilon}\gamma_\tau dB_\tau.
%% \end{align*}
%% The solution to the OU process is
%% \begin{align*}
%% \varepsilon_{\tau} = e^{-\phi\tau}\left(\varepsilon_0 + \int_0^\tau \sigma_{\varepsilon}e^{\phi s}dB_s\right)
%% \end{align*}
%% so that 
%% \begin{align*}
%% \varepsilon_{\tau + \delta} | \varepsilon_{\tau} \sim N\left(e^{-\phi\delta}\varepsilon_{\tau}, \frac{\sigma_{\varepsilon}^2}{2\phi}\left[1 - e^{-2\phi\delta}\right]\right).
%% \end{align*}
%% and thus
%% \begin{align*}
%% \log \gamma_{\tau + \delta}|\log\gamma_\tau \sim N\left(e^{-\phi\delta}(\log\gamma_{\tau} - \mu_\tau) + \mu_{\tau + \delta}, \frac{\sigma_{\varepsilon}^2}{2\phi}\left[1 - e^{-2\phi\delta}\right]\right).
%% \end{align*}
%% In other words, $\gamma_{\tau + \delta}$ has a lognormal distribution. 

%% To stitch this piece onto the larger model, we exploit the inifnite divisibility property of the gamma distribution. Suppose that
%% \begin{align*}
%% x_{\tau + \delta} - x_\tau &\sim G(\beta \int_{\tau}^{\tau + \delta} d\gamma_s, \beta)
%% \end{align*}
%% with $d\gamma_\tau$ defined as above. Since both $x_{\tau}$ and $\gamma_{\tau}$ are continuous time processes, it should be possible to break the interval $[\tau, \tau + \delta]$ into a set of subintervals $[\tau + \delta_k, \tau + \delta_{k+1}]$, $0=\delta_0<\delta_1<\dots<\delta_K=\delta$ such that $x_{\tau + \delta} - x_{\tau} = \sum_{k=1}^K (x_{\tau + \delta_k} - x_{\tau + \delta_{k-1}})$. Since $\int_{\tau}^{\tau + \delta}d\gamma_s = \gamma_{\tau + \delta} - \gamma_{\tau}$ we have 
%% \begin{align*}
%% x_{\tau + \delta_k} - x_{\tau + \delta_{k-1}} \stackrel{ind}{\sim} G(\beta(\gamma_{\tau + \delta_k} - \gamma_{\tau + \delta_{k-1}}), \beta)
%% \end{align*}
%% so that by the gamma distribution's infinite divisibility property
%% \begin{align*}
%% \sum_{k=1}^Kx_{\tau + \delta_k} - x_{\tau + \delta_{k-1}} \sim G\left(\beta\sum_{k=1}^K(\gamma_{\tau + \delta_k} - \gamma_{\tau + \delta_{k-1}}), \beta\right) = G\left(\beta(\gamma_{\tau + \delta} - \gamma_\tau), \beta\right).
%% \end{align*}
%% In other words, this is consistent with the assumed distribution on $x_{\tau + \delta} - x_{\tau}$.

%% The full model can then be written as
%% \begin{align*}
%% t_k|t_{1:(k-1)}, \tau_{1:K} &\sim LN(\log(\tau_k) - \sigma^2/2, \sigma)\mathbbm{1}(t_k > m_k)\\
%% x_{\tau_{k+1}} - x_{\tau_{k}}|\tau_{1:K} & \stackrel{ind}{\sim} G\left(\beta(\gamma_{\tau_k + \delta_k} - \gamma_{\tau_k}),\beta\right)\\
%% \gamma_{\tau_k + \delta_k}|\gamma_{\tau_k} & \sim LN\left(e^{-\phi\delta}(\log\gamma_{\tau_k} - \mu_{\tau_k}) + \mu_{\tau_k + \delta_k}, \frac{\sigma_{\varepsilon}^2}{2\phi}\left[1 - e^{-2\phi\delta}\right]\right)\\
%% \delta_{k} = \tau_{k+1} - \tau_{k} &\stackrel{iid}{\sim} Exp(1/10)
%% \end{align*}
%% for blocks $k=1,2,\dots,N$ where $\mu_{\tau}$ is a parameterized deterministic function of $\tau$. For example, a trend with seasonal variability can be obtained via $\mu_{\tau} = \alpha_0 + \alpha_1\tau + \alpha_2\sin(2\pi f \tau + \psi)$ where $alpha_{0}$, $\alpha_1$, $\alpha_2$, and $\psi$ are uknown parameters and $f$ is the known frequency of the seasonal effect.

Given\footnotetext{Cite RStan} an approximate simulation of the posterior distribution of the model parameters, we can use Monte Carlo simulation to estimate the probability of a block filling up in a number of ways. The simplest way is to compute parameter estimates based on the posterior, e.g. posterior means or posterior medians, then estimate the probability of a block filling up by simulating the model conditional on these parameter values. In order to take into account increased Bitcoin usage, we can manually increase the mean transaction data rate parameter, $\gamma$, while holding other parameters constant. In order to take into account seasonality, we can make $\gamma$ change in a predictable, seasonal pattern.

This approach has two major shortfalls: 1) it ignores parameter uncertainty, and 2) it assumes that the other parameters will remain constant as $\gamma$ increases. We can mitigate 1) while making 2) if not worse, different. Specifically, given access to an approximate simulation of the posterior of the model parameters, we can simulate the model for each draw from the posterior in order to estimate the probability of a block filling up --- this is the posterior predictive distribution.\footnote{Cite Gelman et al. text here} In order to allow for different transaction rates in this simulation, we can set $\gamma$ at a particular value while using the full posterior of the other model parameters, or even better we can shift the posterior of $\gamma$ so that the posterior mean of the shifted posterior is a desired value. This approach takes into account parameter uncertainty but assumes that our uncertainty about the other parameters would be unchanged if the location of $\gamma$'s distribution were different. We estimate the probability of a block filling using all three approaches, but because of the required assumptions the estimates become less credible for values of $\gamma$ or $\mathrm{E}[\gamma]$ further away from the estimated values.

 Table \ref{tab:probsims} contains estimates of the probability of a filled block under a variety of assumptions, using parameter estimates of the posterior distribution of the model using 1000 contiguous blocks ending with block 303151. Probabilities in the first row were computed exactly using the cdf of the exponential distribution. Probabilities in the second and third rows were computed by simulating 800,000 blocks from the model at the specified values for $\gamma$ and $\beta$, where $\beta^{(0.5)}$ denotes the posterior median for $\beta$. Probabilities in the fourth row were computed by simulating 100 blocks from the model for each of the 8000 draws of from the posterior of $\beta$ and with $\gamma$ fixed at the specified values. Probabilities in the fifth and final row were computed by simulating 100 blocks for each of the 8000 draws from the posterior of $\beta$ and the shifted posterior of $\gamma$, where a draw from the shifted posterior of $\gamma$ is a draw from the posterior plus the difference between the specified $\mathrm{E}[\gamma]$ and $\gamma$'s posterior mean. 
% latex table generated in R 3.2.2 by xtable 1.7-4 package
% Tue Oct 13 00:53:54 2015
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrrrr}
  \hline
$E[\gamma] =$ & 0.02 & 0.03 & 0.04 & 0.05 & 0.06 & 0.07 & 0.08 & 0.09 \\ 
  \hline
$\beta = \infty$ & 0.0067 & 0.0357 & 0.0821 & 0.1353 & 0.1889 & 0.2397 & 0.2865 & 0.3292 \\ 
  $\beta = 3\times\beta^{(0.5)}$ & 0.0082 & 0.0387 & 0.0853 & 0.1383 & 0.1915 & 0.2420 & 0.2874 & 0.3293 \\ 
  $\beta = \beta^{(0.5)}$ & 0.0117 & 0.0446 & 0.0917 & 0.1435 & 0.1958 & 0.2446 & 0.2881 & 0.3300 \\ 
  $\beta \sim p(\beta|x,t)$ & 0.0119 & 0.0452 & 0.0919 & 0.1437 & 0.1954 & 0.2438 & 0.2892 & 0.3304 \\ 
  $(\beta,\gamma) \sim p(\beta,\gamma|x,t)$ & 0.0118 & 0.0448 & 0.0916 & 0.1433 & 0.1955 & 0.2442 & 0.2896 & 0.3301 \\ 
   \hline
\end{tabular}
\caption{Probability of a full block under a variety of parameter values. In the first row the transaction rate is constant, in the second through fourth rows the transaction rate is stochastic with fixed mean $\gamma$, and in the final row the transaction rate is stochastic and unknown with mean $\mathrm{E}[\gamma]$. Under all five regimes the expected mean transaction rate is the same --- it is the spread of the distribution of transaction rates and mean transaction rates that changes across regimes.}
\label{tab:probsims}
\end{table}

Under a fixed mean transaction rate, the probability of a full block increases as as $\beta$ decreases, especially for smaller mean transaction rates. For larger mean transaction rates there is little effect on the probability the next block is full from decreasing $\beta$. This is expected since increasing the spread of transaction rates while holding the mean constant increases the variance of block sizes. When block sizes are on average much smaller than 1 MB, increasing the variance of block sizes puts significantly more probability mass above 1 MB, but when the mean is near but still below 1 MB it has a relatively small effect. What is unexpected is that taking into account the uncertainty in $\beta$ and then in both $\gamma$ and $\beta$ seems to have essentially no effect on the probability that the next block fills up no matter the expected value of $\gamma$.

These estimates seem rather high considering no block have filled up yet. Transaction rates have been fluctuating around 0.05 for several months and even under constant transaction rates, the probability of a full block is over 0.1. Even at a constant transaction rate of 0.02, the probability of a full block is 0.067. In 100 blocks, which is approximately 17 hours, the probability of at least one block filling up is $1-(1-0.067)^{100}\approx 1$. Since no block has filled up yet, clearly something is happening that prevents blocks from filling up. One possibility is that transaction rates are usually low and in no danger of causing a full block, but occasionally there are large spikes in the rate. Another possibility is strategic action by Bitcoin traders and miners to prevent blocks from filling up. Traders may be waiting to trade until a new block replaces a nearly full block, which is not much of a cost since the wait is only about 10 minutes on average. Similarly miners have an incentive to keep blocks small since that reduces the change of an orphan block --- a mined block which is not accepted by the blockchain --- so the miner of the current block may be rejecting some transactions in order to prevent the limit from being reached.

\end{document}
