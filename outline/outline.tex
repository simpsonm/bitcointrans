\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{bm, bbm}
%\usepackage{mathtools}

\author{Matt Simpson}
\title{Outline of Ideas for BTC Transaction Frequency}
\begin{document}
\maketitle

\abstract{As Bitcoin's popularity rises, so does the number of transactions per block. Existing Bitcoin rules limit the number of transactions per block to 1 megabytesâ€™ worth. Recent average transaction rates have been about 40 percent of that limit. But because Bitcoin blocks are discovered according to a stochastic Poisson process, some blocks may reach the limit even if the average is well below the limit. This paper models the percentage of blocks that reach the 1MB limit under various average transaction rates. In our most basic model, we derive a formula for that percentage as a function of constant transaction rates. We also estimate how that percentage changes when transaction rates exhibit minute-to-minute variability along specified, empirically defensible distributions.}

\section{The Basic Problem}
Bitcoin blocks are by design distributed according a Poisson process with a rate of discovery of one block every 10 minutes. Let $t$ index time in minutes, and $N(t)$ denote the number of blocks discovered by time $t$. Then for $t>s$, the number of blocks discovered between time $s$ and time $t$ is Poisson distributed, that is $N(t) - N(s) \sim Poi((t-s)/10)$. Let $k=1,2,\dots$ denote each of the blocks, in order. Then the amount of time between blocks is exponentially distributed. Let $\tau_k$ denote the the time when block $k$ arrives. Then $\tau_k - \tau_{k-1} \sim exp(1/10)$. This part of the problem is determined for us, but determining the how the number of transactions is distributed or, more precisely, how the amount of transaction {\it data} is distributed requires more work. Suppose the amount of transaction data in MBs is distributed according to some stochastic process $dx_t$ so that the total amount of data between time $s$ and time $t$, $t>s$, is the stochastic integral $x_t - x_s = \int_s^tdx_t$. Then the amount of transaction data that arrives in $k$'th block is given by $\int_{\tau_k}^{\tau_{k+1}}dx_t$. The quantity we are interested in is thus
\[
P\left(\int_{\tau_k}^{\tau_{k+1}}dx_t > 1\right)
\]
for some choice of $k$. If we discretize the processes so that $t=1,2,\dots$ denotes the minute, this becomes
\[
P\left(\sum_{t=\tau_{k}}^{\tau_{k+1}}dx_t > 1\right).
\]
For some distributional forms for the process $dx_t$ it will be possible to compute these quantities analytically, but in general it may be difficult. Using a Monte Carlo simulation of $N$ draws, we can approximate these probabilities by
\[
\frac{1}{N}\sum_{i=1}^N\mathbbm{1}\left\{\sum_{t=\tau_{k}^{(i)}}^{\tau_{k+1}^{(i)}}dx_{t}^{(i)} > 1\right\}
\]
where $i$ indexes the Monte Carlo simulations of each of the random quantities inside the indicator function. 

The only piece we need to do this is the distribution of the $dx_t$'s. We have two major options here --- we can choose prespecified distributions in order to see how different distributional assumptions affect these probabilities, or we can attempt to estimate the distribution of the transaction data. The former is easy and worth doing just to gather some idea of how different forms of uncertainty affect these probabilities. In the easiest case, we can assume that the $dx_t$'s are fixed at a mean rate of data arrival and analytically compute the probability that a block fills up, showing how this changes depending on the mean data rate. In the most complicated case, we can specify a dynamic model which includes seasonality and other forms of temporal dependence with, again, some fixed mean rate.

Ideally, we would like to fit this sort of model to the BTC transaction data. Doing so will give us more a more empirically plausible distribution with which to compute the probability that a block fills up. Depending on the model, we may be able to predict the growth of BTC transaction data as a side effect, and this may be interesting in its own right.

\section{Simulation Exercises}

Each simulation exercise starts with specifying a distribution for $dx_t$. There are three basic categories here: deterministic, i.e. fixed at the mean rate, independent, and temporally dependent. There is actually some interesting room in the deterministic class --- for example, $dx_t$ could be increasing in $t$, or at least not constant. This behavior can be captured in a more realistic stochastic environment, so we will focus on that. 

There are several possibilities in the independent class. Transaction data amounts are nonnegative, so it is natural to assume that $dx_t \stackrel{iid}{\sim}\mathcal{G}(\alpha,\beta)$ where $\mathcal{G}(\alpha,\beta)$ denotes the gamma distribution with shape and rate parameters $\alpha$ and $\beta$ respectively. Similarly a log-normal distribution is natural, e.g. $\log(dx_t) \stackrel{iid}{\sim}\mathcal{N}(\mu,\sigma^2)$. The log-$T$ distribution is a useful extension of the log-normal. However, all of these distributions encode some degree of right-skewness which may not match the BTC transaction data. A useful generalization to account for all possible sorts of skew is the skew-normal or more generally skew-$T$ distribution. The skew-normal distribution has the pdf
\begin{align*}
p(x) = \frac{2}{\sigma}\phi\left(\frac{x-\mu}{\sigma}\right)\Phi\left(\frac{\alpha(x-\mu)}{\sigma}\right)
\end{align*}
where $\phi(.)$ is the standard normal pdf and $\Phi(.)$ is the standard normal cdf. The parameter $\alpha$ controls the degree and direction of skewness, while $\mu$ and $\sigma$ are respectively location and scale parameters. The definition of the skew-$T$ distribution is analogous. This is only one class of options that could be used to account for possible skewness in the distribution of BTC transaction data.

With temporally dependent data, things start to get more interesting. I think a crucial feature here is that the mean structure be easily separable from the structure of the temporal dependence, that way we can easily ask questions like ``if the mean transaction rate increases to X given this dependence structure, what happens to the probability that a given block fills up?'' So the classic decomposition of a time series into trend, seasonal, and error components is exactly what we are looking for. To wit:
\begin{align*}
log(dx_t) = T_t + S_t + E_t
\end{align*}
where $T_t$ is a deterministic trend component, $S_t$ is a seasonal component which may be stochastic, and $E_t$ is a temporally dependent error component. In a particularly simple case $T_t = \mu$ and $E_t = \rho E_{t-1} + \varepsilon_t$ where $\varepsilon_t \stackrel{iid}{\sim} \mathcal{N}(0,\sigma^2)$, i.e. an AR(1) model. Any mean zero ARMA model can be used for $E_t$, or a plethora of other things, and the error term can also be changed to something skewed skewed if necessary. I suspect the most interesting part for us is the seasonal component --- seasonal variation over the time of day, week, month, or even year may cause certain time periods to have a very high probability of filling up the block. I have a less firm grasp on what I want to try here mostly because I have less experience with modeling seasonal effects, but this does seem like payoff portion of modeling the temporal dependence. 

Another payoff of seasonal effects term is, combined with the trend term, we can predict when in the future BTC will start to have trouble with blocks filling up under a given regime. Also note that we can make these predictions with {\it any} block size substituted in for the actual block size. This is probably not a fruitful exercise unless we build a model specifically with forecasting in mind though.

\section{Fitting Models}
Instead of choosing parameter values and doing the simulation exercise using the models listed in the previous section, we can estimate the models and use simulations form the posterior predictive distribution in order to do the same exercises while not just using more plausible parameter values, but also taking into account parameter uncertainty. A little more formally, suppose our parameter vector is $\bm{\theta}$ and $\bm{y}$ denotes our data. Let $\bm{y}^{pred}$ denote vector predicted observations. The posterior preidctive density for $\bm{y}^{pred}$ is given by
\begin{align*}
p(\bm{y}^{pred}|\bm{y}) = \int p(\bm{y}^{pred}|\bm{\theta})p(\bm{\theta}|\bm{y}) d\bm{\theta}
\end{align*}
where $p(y|x)$ denotes the conditional density of the random variable $y$ given the random variable $x$. Given a simulation from $p(\bm{\theta}|\bm{y})$, the posterior distribution of the model parameters, using e.g. MCMC, this density is easy to approximate simply by simulating draws of the predicted $\bm{y}^{pred}$ for each parameter value simulated. More generally, we can fix some of the parameter values and let others vary to, e.g., isolate the effect of increasing the mean of the time series. 

Fitting the models should be straightforward, at least in theory. STAN is probably great for what we are trying to do, which means way less work on our end. The only wrinkles here are 1) now we have to pick prior distributions, which may be controversial, but we can easily do some robustness checks with different priors and 2) potential big data concerns.

\section{A Simple Example}

In this example simulation we assume that the transaction data process $x_t$ follows a continuous time random walk with drift with parameters $\mu$ and $\sigma$, that is
\[
x_t - x_s \sim N((t-s)\mu, (t-s)\sigma^2)
\]
for $t > s$. So $dx_t = \mu dt + \sigma d B_t$ where $B_t$ is Brownian motion. Using a simulation, Table \ref{tab:bmprob} contains Monte Carlo estimates of the probability that a block fills up under a variety of choices of $\mu$ and $\sigma$. The current average transaction data rate per minute  is $\mu=0.04$ MBs. As $\sigma$ increases the probability of a block filling increases, though when $\sigma$ is close to zero an increase in $\sigma$ may cause the probability to decrease slightly. For most of the range, however, increasing $\sigma$ increases the probability that a block fills up. 

% latex table generated in R 3.2.2 by xtable 1.7-4 package
% Tue Sep 22 16:26:38 2015
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrrrr}
  \hline
 & $\sigma = 0$ & $\sigma = 0.001$ & $\sigma = 0.01$ & $\sigma = 0.1$ & $\sigma = 1$ & $\sigma = 10$ & $\sigma = 100$ & $\sigma = 1000$ \\ 
  \hline
$\mu = 0.035$ & 0.0594 & 0.0594 & 0.0597 & 0.0927 & 0.3574 & 0.4822 & 0.4979 & 0.4994 \\ 
  $\mu = 0.04$ & 0.0836 & 0.0835 & 0.0839 & 0.1149 & 0.3624 & 0.4824 & 0.4978 & 0.5005 \\ 
  $\mu = 0.045$ & 0.1098 & 0.1098 & 0.1103 & 0.1367 & 0.3680 & 0.4829 & 0.4977 & 0.4995 \\ 
  $\mu = 0.05$ & 0.1366 & 0.1366 & 0.1370 & 0.1596 & 0.3739 & 0.4837 & 0.4983 & 0.5003 \\ 
   \hline
\end{tabular}
\caption{Approximate probability of filling of a block under various choices of $\mu$ and $\sigma$.}
\label{tab:bmprob}
\end{table}

\end{document}
